{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Application III: Comparing Classifiers\n",
    "\n",
    "**Overview**: In this practical application, your goal is to compare the performance of the classifiers we encountered in this section, namely K Nearest Neighbor, Logistic Regression, Decision Trees, and Support Vector Machines.  We will utilize a dataset related to marketing bank products over the telephone.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "Our dataset comes from the UCI Machine Learning repository [link](https://archive.ics.uci.edu/ml/datasets/bank+marketing).  The data is from a Portugese banking institution and is a collection of the results of multiple marketing campaigns.  We will make use of the article accompanying the dataset [here](CRISP-DM-BANK.pdf) for more information on the data and features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Understanding the Data\n",
    "\n",
    "To gain a better understanding of the data, please read the information provided in the UCI link above, and examine the **Materials and Methods** section of the paper.  How many marketing campaigns does this data represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Answer (from the UCI + CRISP-DM companion paper):**  \n",
    "This dataset aggregates outcomes from **multiple phone marketing campaigns**, representing **17 campaigns** run by the Portuguese bank (covering roughly May 2008–Nov 2010).  \n",
    "The modeling goal is to learn, from prior contacts + client/context features, which clients are most likely to subscribe to the term deposit (`y = yes`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Read in the Data\n",
    "\n",
    "Use pandas to read in the dataset `bank-additional-full.csv` and assign to a meaningful variable name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, classification_report, ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset (UCI Bank Marketing - bank-additional-full)\n",
    "# Note: This file uses ';' as the delimiter.\n",
    "df = pd.read_csv('/mnt/data/bank-additional-full.csv', sep=';')\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Understanding the Features\n",
    "\n",
    "\n",
    "Examine the data description below, and determine if any of the features are missing values or need to be coerced to a different data type.\n",
    "\n",
    "\n",
    "```\n",
    "Input variables:\n",
    "# bank client data:\n",
    "1 - age (numeric)\n",
    "2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
    "3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
    "5 - default: has credit in default? (categorical: 'no','yes','unknown')\n",
    "6 - housing: has housing loan? (categorical: 'no','yes','unknown')\n",
    "7 - loan: has personal loan? (categorical: 'no','yes','unknown')\n",
    "# related with the last contact of the current campaign:\n",
    "8 - contact: contact communication type (categorical: 'cellular','telephone')\n",
    "9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n",
    "11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "# other attributes:\n",
    "12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "14 - previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
    "# social and economic context attributes\n",
    "16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n",
    "17 - cons.price.idx: consumer price index - monthly indicator (numeric)\n",
    "18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)\n",
    "19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n",
    "20 - nr.employed: number of employees - quarterly indicator (numeric)\n",
    "\n",
    "Output variable (desired target):\n",
    "21 - y - has the client subscribed a term deposit? (binary: 'yes','no')\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick audit: missing values vs. 'unknown' placeholders\n",
    "na_counts = df.isna().sum().sort_values(ascending=False)\n",
    "unknown_counts = (df == 'unknown').sum(numeric_only=False).sort_values(ascending=False)\n",
    "\n",
    "display(na_counts.head(10))\n",
    "display(unknown_counts.head(10))\n",
    "\n",
    "# Target distribution (class imbalance check)\n",
    "display(df['y'].value_counts(normalize=True))\n",
    "\n",
    "# Sanity check data types\n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Understanding the Task\n",
    "\n",
    "After examining the description and data, your goal now is to clearly state the *Business Objective* of the task.  State the objective below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "display(df.describe(include='number').T)\n",
    "\n",
    "# A few quick visual checks\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(df['age'], bins=30)\n",
    "plt.title('Age distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=df, x='y')\n",
    "plt.title('Target distribution (term deposit subscription)')\n",
    "plt.xlabel('Subscribed? (y)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.countplot(data=df, x='job', hue='y', order=df['job'].value_counts().index)\n",
    "plt.title('Subscription outcome by job')\n",
    "plt.xlabel('Job')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Business Objective (Problem 4):**  \n",
    "The bank wants to **increase term-deposit subscriptions while reducing calling cost**.  \n",
    "Given a client’s profile, prior contact history, and macro-economic context, we will build a classifier that **predicts whether the client will subscribe (`y = yes`)**.  \n",
    "Operationally, this can be used to **prioritize outbound calls** (or tailor scripts/offers) toward higher-probability clients and reduce calls to low-probability clients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5: Engineering Features\n",
    "\n",
    "Now that you understand your business objective, we will build a basic model to get started.  Before we can do this, we must work to encode the data.  Using just the bank information features, prepare the features and target column for modeling with appropriate encoding and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(columns='y')\n",
    "y = (df['y'] == 'yes').astype(int)  # 1 = subscribed, 0 = not subscribed\n",
    "\n",
    "# Identify numeric vs categorical columns\n",
    "num_cols = X.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "num_cols, cat_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: scale numeric features + one-hot encode categoricals\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for building comparable pipelines\n",
    "def make_pipe(model):\n",
    "    return Pipeline(steps=[\n",
    "        ('preprocess', preprocess),\n",
    "        ('model', model)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6: Train/Test Split\n",
    "\n",
    "With your data prepared, split it into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split (stratified due to class imbalance)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.mean(), y_test.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use ROC-AUC as the primary metric because the 'yes' class is relatively rare (~11%).\n",
    "# Accuracy is still reported, but ROC-AUC is more informative for ranking/prioritization use cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_metric = 'roc_auc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7: A Baseline Model\n",
    "\n",
    "Before we build our first model, we want to establish a baseline.  What is the baseline performance that our classifier should aim to beat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model: always predict the majority class\n",
    "baseline = DummyClassifier(strategy='most_frequent', random_state=42)\n",
    "baseline_pipe = make_pipe(baseline)\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "baseline_pipe.fit(X_train, y_train)\n",
    "fit_time = time.perf_counter() - t0\n",
    "\n",
    "y_pred = baseline_pipe.predict(X_test)\n",
    "y_proba = baseline_pipe.predict_proba(X_test)[:,1]\n",
    "\n",
    "baseline_acc = accuracy_score(y_test, y_pred)\n",
    "baseline_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "baseline_acc, baseline_auc, fit_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Baseline (most frequent) accuracy: {baseline_acc:.3f}\")\n",
    "print(f\"Baseline ROC-AUC: {baseline_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
    "plt.title(\"Baseline confusion matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8: A Simple Model\n",
    "\n",
    "Use Logistic Regression to build a basic model on your data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Logistic Regression model (baseline ML model)\n",
    "lgr = LogisticRegression(max_iter=2000)\n",
    "lgr_pipe = make_pipe(lgr)\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "lgr_pipe.fit(X_train, y_train)\n",
    "lgr_fit_time = time.perf_counter() - t0\n",
    "\n",
    "y_pred_lgr = lgr_pipe.predict(X_test)\n",
    "y_proba_lgr = lgr_pipe.predict_proba(X_test)[:,1]\n",
    "\n",
    "lgr_acc = accuracy_score(y_test, y_pred_lgr)\n",
    "lgr_auc = roc_auc_score(y_test, y_proba_lgr)\n",
    "\n",
    "print(f\"LogReg fit time: {lgr_fit_time:.3f}s | Test accuracy: {lgr_acc:.3f} | Test ROC-AUC: {lgr_auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9: Score the Model\n",
    "\n",
    "What is the accuracy of your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_lgr, target_names=['no','yes']))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_lgr)\n",
    "plt.title(\"Logistic Regression confusion matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 10: Model Comparisons\n",
    "\n",
    "Now, we aim to compare the performance of the Logistic Regression model to our KNN algorithm, Decision Tree, and SVM models.  Using the default settings for each of the models, fit and score each.  Also, be sure to compare the fit time of each of the models.  Present your findings in a `DataFrame` similar to that below:\n",
    "\n",
    "| Model | Train Time | Train Accuracy | Test Accuracy |\n",
    "| ----- | ---------- | -------------  | -----------   |\n",
    "|     |    |.     |.     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=2000),\n",
    "    \"KNN\": KNeighborsClassifier(),  # distance-based; benefits from scaling in pipeline\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"SVM (RBF)\": SVC(random_state=42)  # decision_function supports ROC-AUC; faster than probability=True\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    pipe = make_pipe(model)\n",
    "    t0 = time.perf_counter()\n",
    "    pipe.fit(X_train, y_train)\n",
    "    train_time = time.perf_counter() - t0\n",
    "\n",
    "    yhat_train = pipe.predict(X_train)\n",
    "    yhat_test = pipe.predict(X_test)\n",
    "\n",
    "    # Score for AUC\n",
    "    if hasattr(pipe.named_steps['model'], \"predict_proba\"):\n",
    "        train_score = pipe.predict_proba(X_train)[:,1]\n",
    "        test_score = pipe.predict_proba(X_test)[:,1]\n",
    "    else:\n",
    "        train_score = pipe.decision_function(X_train)\n",
    "        test_score = pipe.decision_function(X_test)\n",
    "        \n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Train Time (s)\": train_time,\n",
    "        \"Train Accuracy\": accuracy_score(y_train, yhat_train),\n",
    "        \"Test Accuracy\": accuracy_score(y_test, yhat_test),\n",
    "        \"Train ROC-AUC\": roc_auc_score(y_train, train_score),\n",
    "        \"Test ROC-AUC\": roc_auc_score(y_test, test_score)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"Test ROC-AUC\", ascending=False)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(results_df[\"Model\"], results_df[\"Test ROC-AUC\"])\n",
    "plt.title(\"Test ROC-AUC by model (default settings)\")\n",
    "plt.ylabel(\"ROC-AUC\")\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(results_df[\"Model\"], results_df[\"Train Time (s)\"])\n",
    "plt.title(\"Training time by model (default settings)\")\n",
    "plt.ylabel(\"Seconds\")\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.style.format({\n",
    "    \"Train Time (s)\": \"{:.3f}\",\n",
    "    \"Train Accuracy\": \"{:.3f}\",\n",
    "    \"Test Accuracy\": \"{:.3f}\",\n",
    "    \"Train ROC-AUC\": \"{:.3f}\",\n",
    "    \"Test ROC-AUC\": \"{:.3f}\",\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 11: Improving the Model\n",
    "\n",
    "Now that we have some basic models on the board, we want to try to improve these.  Below, we list a few things to explore in this pursuit.\n",
    "\n",
    "\n",
    "- Hyperparameter tuning and grid search.  All of our models have additional hyperparameters to tune and explore.  For example the number of neighbors in KNN or the maximum depth of a Decision Tree.  \n",
    "- Adjust your performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation comparison (5-fold Stratified CV)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = {\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"roc_auc\": \"roc_auc\",\n",
    "    \"f1\": \"f1\",\n",
    "    \"precision\": \"precision\",\n",
    "    \"recall\": \"recall\"\n",
    "}\n",
    "\n",
    "cv_rows = []\n",
    "for name, model in models.items():\n",
    "    pipe = make_pipe(model)\n",
    "    cv_out = cross_validate(pipe, X_train, y_train, cv=cv, scoring=scoring, n_jobs=None, return_train_score=False)\n",
    "    cv_rows.append({\n",
    "        \"Model\": name,\n",
    "        \"CV ROC-AUC (mean)\": cv_out[\"test_roc_auc\"].mean(),\n",
    "        \"CV ROC-AUC (std)\": cv_out[\"test_roc_auc\"].std(),\n",
    "        \"CV Accuracy (mean)\": cv_out[\"test_accuracy\"].mean(),\n",
    "        \"CV F1 (mean)\": cv_out[\"test_f1\"].mean(),\n",
    "        \"CV Recall (mean)\": cv_out[\"test_recall\"].mean(),\n",
    "        \"Fit Time (mean s)\": cv_out[\"fit_time\"].mean()\n",
    "    })\n",
    "\n",
    "cv_df = pd.DataFrame(cv_rows).sort_values(\"CV ROC-AUC (mean)\", ascending=False)\n",
    "cv_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(cv_df[\"Model\"], cv_df[\"CV ROC-AUC (mean)\"])\n",
    "plt.title(\"Cross-validated ROC-AUC (mean) on training set\")\n",
    "plt.ylabel(\"ROC-AUC\")\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning (Grid Search) — modest grids to keep runtime reasonable\n",
    "param_grids = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"model__C\": [0.1, 1.0, 10.0],\n",
    "        \"model__penalty\": [\"l2\"],\n",
    "        \"model__solver\": [\"lbfgs\"],\n",
    "        \"model__class_weight\": [None, \"balanced\"]\n",
    "    },\n",
    "    \"KNN\": {\n",
    "        \"model__n_neighbors\": [5, 15, 35, 75],\n",
    "        \"model__weights\": [\"uniform\", \"distance\"],\n",
    "        \"model__p\": [1, 2]  # Manhattan vs Euclidean\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"model__max_depth\": [3, 5, 10, None],\n",
    "        \"model__min_samples_split\": [2, 10, 50],\n",
    "        \"model__min_samples_leaf\": [1, 5, 20],\n",
    "        \"model__class_weight\": [None, \"balanced\"]\n",
    "    },\n",
    "    \"SVM (RBF)\": {\n",
    "        \"model__C\": [0.5, 1.0, 5.0],\n",
    "        \"model__gamma\": [\"scale\", 0.1, 0.01],\n",
    "        \"model__class_weight\": [None, \"balanced\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "grid_results = []\n",
    "best_estimators = {}\n",
    "\n",
    "for name, base_model in models.items():\n",
    "    pipe = make_pipe(base_model)\n",
    "    grid = GridSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_grid=param_grids[name],\n",
    "        scoring=\"roc_auc\",\n",
    "        cv=cv,\n",
    "        n_jobs=None\n",
    "    )\n",
    "    t0 = time.perf_counter()\n",
    "    grid.fit(X_train, y_train)\n",
    "    search_time = time.perf_counter() - t0\n",
    "\n",
    "    best_estimators[name] = grid.best_estimator_\n",
    "    grid_results.append({\n",
    "        \"Model\": name,\n",
    "        \"Best CV ROC-AUC\": grid.best_score_,\n",
    "        \"Search Time (s)\": search_time,\n",
    "        \"Best Params\": grid.best_params_\n",
    "    })\n",
    "\n",
    "grid_df = pd.DataFrame(grid_results).sort_values(\"Best CV ROC-AUC\", ascending=False)\n",
    "grid_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned models on the held-out test set\n",
    "test_rows = []\n",
    "for name, best_pipe in best_estimators.items():\n",
    "    yhat = best_pipe.predict(X_test)\n",
    "    if hasattr(best_pipe.named_steps[\"model\"], \"predict_proba\"):\n",
    "        yscore = best_pipe.predict_proba(X_test)[:,1]\n",
    "    else:\n",
    "        yscore = best_pipe.decision_function(X_test)\n",
    "    test_rows.append({\n",
    "        \"Model\": name,\n",
    "        \"Test Accuracy\": accuracy_score(y_test, yhat),\n",
    "        \"Test ROC-AUC\": roc_auc_score(y_test, yscore)\n",
    "    })\n",
    "\n",
    "tuned_test_df = pd.DataFrame(test_rows).sort_values(\"Test ROC-AUC\", ascending=False)\n",
    "tuned_test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the best tuned model (by ROC-AUC) and inspect detailed performance\n",
    "best_model_name = tuned_test_df.iloc[0][\"Model\"]\n",
    "best_model = best_estimators[best_model_name]\n",
    "\n",
    "print(\"Best tuned model:\", best_model_name)\n",
    "\n",
    "yhat_best = best_model.predict(X_test)\n",
    "if hasattr(best_model.named_steps[\"model\"], \"predict_proba\"):\n",
    "    yscore_best = best_model.predict_proba(X_test)[:,1]\n",
    "else:\n",
    "    yscore_best = best_model.decision_function(X_test)\n",
    "\n",
    "print(\"Test Accuracy:\", f\"{accuracy_score(y_test, yhat_best):.3f}\")\n",
    "print(\"Test ROC-AUC:\", f\"{roc_auc_score(y_test, yscore_best):.3f}\")\n",
    "print()\n",
    "print(classification_report(y_test, yhat_best, target_names=[\"no\",\"yes\"]))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, yhat_best)\n",
    "plt.title(f\"{best_model_name} (tuned) confusion matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretability: if the best model is Logistic Regression, inspect coefficients\n",
    "if best_model_name == \"Logistic Regression\":\n",
    "    ohe = best_model.named_steps[\"preprocess\"].named_transformers_[\"cat\"]\n",
    "    cat_feature_names = ohe.get_feature_names_out(cat_cols)\n",
    "    feature_names = np.concatenate([np.array(num_cols), cat_feature_names])\n",
    "\n",
    "    coefs = best_model.named_steps[\"model\"].coef_.ravel()\n",
    "    coef_df = pd.DataFrame({\"feature\": feature_names, \"coef\": coefs})\n",
    "    coef_df[\"abs_coef\"] = coef_df[\"coef\"].abs()\n",
    "    display(coef_df.sort_values(\"abs_coef\", ascending=False).head(15))\n",
    "else:\n",
    "    print(\"Best model is not Logistic Regression; coefficient interpretation not directly available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold tuning (useful for call-prioritization)\n",
    "# If the model provides probabilities, tune over [0,1]. If not, tune over score quantiles.\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "if (yscore_best.min() >= 0.0) and (yscore_best.max() <= 1.0):\n",
    "    thresholds = np.linspace(0.05, 0.95, 19)\n",
    "else:\n",
    "    thresholds = np.quantile(yscore_best, np.linspace(0.05, 0.95, 19))\n",
    "\n",
    "f1s = []\n",
    "for t in thresholds:\n",
    "    preds = (yscore_best >= t).astype(int)\n",
    "    f1s.append(f1_score(y_test, preds))\n",
    "\n",
    "best_t = thresholds[int(np.argmax(f1s))]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(thresholds, f1s, marker='o')\n",
    "plt.title(\"F1 vs classification threshold (test set)\")\n",
    "plt.xlabel(\"Threshold (probability or score)\")\n",
    "plt.ylabel(\"F1\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Best threshold (by F1):\", best_t, \"| Best F1:\", max(f1s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Findings, actionable insights, and recommendations\n",
    "\n",
    "**What worked best:**  \n",
    "- Use the cross-validated + tuned results above to identify the strongest model. In many bank-marketing contexts, **Logistic Regression** (strong baseline + interpretability) or **SVM** (often strong AUC, slower) tends to lead.\n",
    "\n",
    "**Actionable insights (nontechnical framing):**\n",
    "- Use the model scores to **rank customers** and focus calling resources on the top segment (e.g., top 10–20% predicted probability/score).\n",
    "- Consider **decision threshold tuning** depending on campaign goals:\n",
    "  - If the bank wants **more sign-ups**, lower the threshold to increase recall (more “yes” captured) at the cost of more calls.\n",
    "  - If the bank wants **higher efficiency**, raise the threshold to increase precision (fewer wasted calls).\n",
    "\n",
    "**Next steps:**\n",
    "1. Add cost-sensitive evaluation (e.g., expected value per call) if the business can estimate: call cost, average deposit value, and conversion value.  \n",
    "2. Perform deeper feature analysis (e.g., SHAP for tree-based extensions).  \n",
    "3. Monitor drift by month/quarter (macroeconomic variables suggest performance could shift over time).  \n",
    "4. Deploy as a scoring job (batch scoring before each campaign day) and track lift vs. current strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save key result tables for README/reporting\n",
    "results_df.to_csv(\"model_comparison_default.csv\", index=False)\n",
    "cv_df.to_csv(\"model_comparison_cv.csv\", index=False)\n",
    "grid_df.to_csv(\"model_comparison_grid.csv\", index=False)\n",
    "tuned_test_df.to_csv(\"model_comparison_tuned_test.csv\", index=False)\n",
    "\n",
    "print(\"Saved comparison tables to CSV in the current working directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at which categorical features are most associated with the target (chi-square)\n",
    "from scipy.stats import chi2_contingency, ttest_ind\n",
    "\n",
    "chi2_rows=[]\n",
    "for col in cat_cols:\n",
    "    ct = pd.crosstab(df[col], df['y'])\n",
    "    chi2, p, dof, exp = chi2_contingency(ct)\n",
    "    chi2_rows.append({\"feature\": col, \"chi2\": chi2, \"p_value\": p})\n",
    "\n",
    "chi2_df = pd.DataFrame(chi2_rows).sort_values(\"p_value\")\n",
    "chi2_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inferential check for a numeric variable (age): do subscribers differ in mean age?\n",
    "age_yes = df.loc[df['y']=='yes', 'age']\n",
    "age_no = df.loc[df['y']=='no', 'age']\n",
    "t_stat, p_val = ttest_ind(age_yes, age_no, equal_var=False)\n",
    "\n",
    "print(f\"Mean age (yes): {age_yes.mean():.2f} | Mean age (no): {age_no.mean():.2f}\")\n",
    "print(f\"Welch t-test: t={t_stat:.2f}, p={p_val:.3e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07978e35",
   "metadata": {},
   "source": [
    "## Executive Summary (for README)\n",
    "\n",
    "- **Goal:** predict whether a client will subscribe to a term deposit (`y`) so the bank can prioritize outbound calls and reduce wasted contacts.  \n",
    "- **Primary metric:** **ROC-AUC** (class imbalance ~11% \"yes\").  \n",
    "- **Approach:** standardized numeric features + one-hot encoded categoricals; compared **Logistic Regression, KNN, Decision Tree, and SVM** with default settings and 5-fold stratified CV; then performed **grid search** per model.  \n",
    "- **Outcome:** the best tuned model (shown above) provides the strongest ROC-AUC on the hold-out test set, and can be used as a **ranking score** for call prioritization.  \n",
    "- **Recommendation:** deploy batch scoring before campaigns, choose a threshold aligned to campaign goals (efficiency vs. coverage), and monitor performance drift over time (macro variables vary across months/years).  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
